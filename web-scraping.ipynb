{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Mining and Applied NLP (44-620)\n",
    "\n",
    "## Web Scraping and NLP with Requests, BeautifulSoup, and spaCy\n",
    "\n",
    "### Student Name: Jacob Sellinger\n",
    "\n",
    "Perform the tasks described in the Markdown cells below.  When you have completed the assignment make sure your code cells have all been run (and have output beneath them) and ensure you have committed and pushed ALL of your changes to your assignment repository.\n",
    "\n",
    "Every question that requires you to write code will have a code cell underneath it; you may either write your entire solution in that cell or write it in a python file (`.py`), then import and run the appropriate code to answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up\n",
    "1. Set up venv\n",
    "    py -m venv .venv\n",
    "2. activate venv\n",
    "    .venv\\Scripts\\Activate\n",
    "3. Check Path interpreter just in case\n",
    "4. Get to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Python Environment Diagnostics ---\n",
      "Python Executable: c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n",
      "Python Version: 3.13.0 (tags/v3.13.0:60403a5, Oct  7 2024, 09:38:07) [MSC v.1941 64 bit (AMD64)]\n",
      "PYTHONPATH (Environment Variable): Not set\n",
      "sys.path (Interpreter's Search Path):\n",
      "  - c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python313\\python313.zip\n",
      "  - c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python313\\DLLs\n",
      "  - c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python313\\Lib\n",
      "  - c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python313\n",
      "  - \n",
      "  - c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\n",
      "  - c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\win32\n",
      "  - c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\win32\\lib\n",
      "  - c:\\Users\\jacob\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\Pythonwin\n",
      "--- End Diagnostics ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Confirm Path just in case\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"--- Python Environment Diagnostics ---\")\n",
    "print(\"Python Executable:\", sys.executable)\n",
    "print(\"Python Version:\", sys.version)\n",
    "print(\"PYTHONPATH (Environment Variable):\", os.environ.get('PYTHONPATH', 'Not set'))\n",
    "print(\"sys.path (Interpreter's Search Path):\")\n",
    "for p in sys.path:\n",
    "    print(f\"  - {p}\")\n",
    "print(\"--- End Diagnostics ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write code that extracts the article html from https://web.archive.org/web/20210327165005/https://hackaday.com/2021/03/22/how-laser-headlights-work/ and dumps it to a .pkl (or other appropriate file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved successfully to test.json\n"
     ]
    }
   ],
   "source": [
    "#Considering either pickling (.pkl), JSON, or can drop straight into a pandas data frame\n",
    "# Choosiung to drop into JSON because it plenty of examples and non-Python specific\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "#Let's create a function just so I can do this later\n",
    "\n",
    "def web_scrape(url):\n",
    "    response = requests.get(url)\n",
    "    html_text = response.text\n",
    "    web_soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    web_soup_text = web_soup.text\n",
    "\n",
    "    \"\"\"This is returning a continuous string of stripped HTML text\"\"\"\n",
    "    return(web_soup_text)\n",
    "    \n",
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(data, f, indent = 4)\n",
    "    print(f\"file saved successfully to {filename}\")\n",
    "\n",
    "\n",
    "save_to_json(web_scrape(\"https://en.wikipedia.org/wiki/Logging_(computing)\"), \"test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read in your article's html source from the file you created in question 1 and print it's text (use `.get_text()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used a JSON file so this is how I will interact with it\n",
    "\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(data)\n",
    "    return data\n",
    "\n",
    "load_json(\"test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent tokens (converted to lower case).  Print the common tokens with an appropriate label.  Additionally, print the tokens their frequencies (with appropriate labels). Make sure to remove things we don't care about (punctuation, stopwords, whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because I dumped a single long string into a JSON, which is now a Python Dictionary, I need to proces this data\n",
    "# First step to processing the data is pre-process as in making this clean\n",
    "import spacy\n",
    "import spacytextblob\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"Following the natural spacy pipeline of tokenization, lemmatization, removal of junk, tagging, NER\"\n",
    "    #Initialize language and doc\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    #Tokenization\n",
    "    tokens = [token.text.lower() for token in doc]\n",
    "\n",
    "    #Lemmatization\n",
    "    lemmas = [token.lemma_.lower() for token in doc]\n",
    "\n",
    "    #Removing Junk\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "    filtered_tokens = [token.text.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "\n",
    "    #Filtered Lemmas\n",
    "    filtered_lemmas = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "\n",
    "    #Part of Speech Tagging\n",
    "    pos_tags = [(token.text.lower(), token.pos_) for token in doc]\n",
    "\n",
    "    #Named Entity Recognition (NER)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    print(tokens, lemmas, filtered_tokens, filtered_lemmas ,pos_tags, entities)\n",
    "    return tokens, lemmas, filtered_tokens,filtered_lemmas ,pos_tags, entities\n",
    "\n",
    "\n",
    "example = load_json(\"test.json\")\n",
    "example_processed_data = preprocess_text(example)\n",
    "\n",
    "\"Next to answer the actual question we will use the collections module\"\n",
    "\"This was suggested by gemini for its efficiency with large data sets.\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def spacy_count(data, top_x):\n",
    "    # Count items\n",
    "    counter = Counter(data)\n",
    "    #Count unique items and occurences\n",
    "    unique_items = dict(counter)\n",
    "    top_x_items = counter.most_common(top_x)\n",
    "\n",
    "    print(unique_items, top_x_items)\n",
    "    return unique_items, top_x_items\n",
    "\n",
    "spacy_count(example_processed_data[2], 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the article text into a trained `spaCy` pipeline, and determine the 5 most frequent lemmas (converted to lower case).  Print the common lemmas with an appropriate label.  Additionally, print the lemmas with their frequencies (with appropriate labels). Make sure to remove things we don't care about (punctuation, stopwords, whitespace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logging': 8, 'computing': 5, 'wikipedia': 5, 'jump': 1, 'content': 6, 'main': 4, 'menu': 2, 'sidebar': 4, 'hide': 4, 'navigation': 1, 'pagecontentscurrent': 1, 'eventsrandom': 1, 'articleabout': 1, 'wikipediacontact': 1, 'contribute': 1, 'helplearn': 1, 'editcommunity': 1, 'portalrecent': 1, 'changesupload': 2, 'filespecial': 1, 'page': 4, 'search': 11, 'appearance': 2, 'donate': 2, 'create': 3, 'account': 2, 'log': 67, 'personal': 1, 'tool': 1, 'pages': 1, 'editor': 1, 'learn': 2, 'contributionstalk': 1, 'types': 2, 'toggle': 3, 'subsection': 1, 'event': 17, 'transaction': 13, 'message': 16, 'server': 19, 'reference': 2, 'table': 2, 'language': 2, 'catalàčeštinadeutschespañolفارسیfrançais한국어հայերենbahasa': 1, 'indonesiaitalianoעבריתқазақшаmagyarnederlands日本語norsk': 1, 'bokmålpolskiportuguêsрусскийsimple': 1, 'englishslovenčinasuomisvenskatürkçeукраїнська粵語中文': 1, 'edit': 2, 'link': 3, 'articletalk': 1, 'english': 1, 'readeditview': 2, 'history': 4, 'tools': 2, 'actions': 1, 'general': 3, 'hererelate': 1, 'filepermanent': 1, 'linkpage': 1, 'informationcite': 1, 'pageget': 1, 'shorten': 1, 'urldownload': 1, 'qr': 1, 'code': 4, 'print': 1, 'export': 1, 'download': 1, 'pdfprintable': 1, 'version': 1, 'project': 1, 'wikimedia': 2, 'commonswikidata': 1, 'item': 1, 'free': 1, 'encyclopedia': 1, 'chronological': 1, 'record': 8, 'computer': 6, 'datum': 6, 'processing': 1, 'operation': 4, 'article': 5, 'need': 3, 'additional': 4, 'citation': 2, 'verification': 1, 'help': 1, 'improve': 1, 'add': 3, 'reliable': 1, 'source': 3, 'unsourced': 1, 'material': 1, 'challenge': 1, 'remove': 2, 'find': 1, 'news': 1, 'newspaper': 1, 'book': 1, 'scholar': 1, 'jstor': 1, 'september': 3, 'confuse': 1, 'data': 4, 'logger': 1, 'act': 1, 'keep': 1, 'occur': 3, 'system': 16, 'problem': 3, 'error': 3, 'broad': 1, 'information': 12, 'current': 1, 'operating': 2, 'software': 9, 'entry': 4, 'monitor': 2, 'understand': 5, 'debug': 1, 'audit': 2, 'particularly': 2, 'important': 1, 'multi': 1, 'user': 16, 'central': 1, 'overview': 1, 'simple': 1, 'case': 3, 'write': 2, 'file': 16, 'call': 1, 'alternatively': 1, 'dedicated': 1, 'management': 5, 'store': 5, 'database': 5, 'different': 4, 'specifically': 1, 'communication': 3, 'collection': 2, 'method': 1, 'automatically': 4, 'capture': 1, 'type': 1, 'time': 5, 'person': 2, 'terminal': 1, 'web': 10, 'searching': 1, 'electronic': 1, 'interaction': 2, 'episode': 1, 'engine': 3, 'framework': 1, 'program': 2, 'include': 3, 'widely': 1, 'standard': 4, 'syslog': 3, 'define': 1, 'ietf': 1, 'rfc': 3, 'enable': 2, 'dedicate': 1, 'standardized': 1, 'subsystem': 1, 'generate': 1, 'filter': 1, 'analyze': 1, 'relieve': 1, 'developer': 2, 'have': 1, 'design': 2, 'ad': 1, 'hoc': 1, 'take': 1, 'place': 1, 'execution': 1, 'activity': 2, 'diagnose': 1, 'essential': 1, 'application': 2, 'little': 1, 'useful': 1, 'combine': 2, 'multiple': 1, 'combination': 1, 'yield': 1, 'relate': 2, 'solution': 1, 'employ': 1, 'network': 2, 'wide': 1, 'querying': 1, 'maintain': 5, 'kind': 1, 'mainly': 1, 'intend': 2, 'trail': 1, 'later': 1, 'analysis': 5, 'human': 2, 'readable': 3, 'change': 5, 'allow': 1, 'recover': 1, 'crash': 1, 'consistent': 1, 'state': 1, 'usually': 2, 'use': 4, 'intranet': 1, 'site': 3, 'provide': 1, 'valuable': 1, 'insight': 1, 'process': 1, 'online': 1, 'understanding': 1, 'enlighten': 1, 'interface': 1, 'development': 2, 'devise': 1, 'architecture': 2, 'internet': 3, 'relay': 1, 'chat': 5, 'irc': 4, 'instant': 2, 'messaging': 2, 'im': 3, 'peer': 2, 'share': 1, 'client': 4, 'function': 1, 'multiplayer': 1, 'game': 1, 'especially': 1, 'mmorpg': 1, 'commonly': 1, 'ability': 1, 'save': 2, 'textual': 2, 'public': 2, 'channel': 4, 'conference': 1, 'mmo': 1, 'party': 1, 'private': 1, 'universally': 1, 'plain': 1, 'text': 3, 'voip': 2, 'support': 1, 'skype': 1, 'html': 1, 'custom': 1, 'format': 6, 'ease': 1, 'read': 1, 'encryption': 1, 'topic': 2, 'join': 1, 'exit': 1, 'kick': 1, 'ban': 1, 'nickname': 1, 'status': 1, 'make': 1, 'like': 1, 'combined': 1, 'question': 1, 'comparable': 1, 'true': 1, 'visible': 1, 'frame': 1, 'spend': 1, 'connect': 1, 'certain': 1, 'offer': 1, 'chance': 1, 'encrypt': 2, 'enhance': 1, 'privacy': 4, 'require': 1, 'password': 1, 'decrypt': 1, 'view': 2, 'handle': 1, 'respective': 1, 'writing': 1, 'focus': 2, 'service': 3, 'signal': 1, 'minimal': 1, 'limit': 1, 'connection': 1, 'apache': 1, 'access': 2, 'show': 1, 'wordpress': 1, 'vulnerability': 1, 'bot': 1, 'consist': 1, 'list': 1, 'perform': 1, 'typical': 1, 'example': 1, 'request': 4, 'common': 1, 'proprietary': 1, 'json': 1, 'versus': 1, 'recent': 1, 'typically': 3, 'append': 1, 'end': 1, 'ip': 1, 'address': 1, 'date': 1, 'http': 1, 'bytes': 1, 'serve': 1, 'agent': 2, 'referrer': 3, 'single': 1, 'separate': 1, 'distinct': 1, 'collect': 1, 'specific': 1, 'accessible': 1, 'webmaster': 1, 'administrative': 1, 'statistical': 1, 'examine': 1, 'traffic': 1, 'pattern': 1, 'day': 2, 'week': 1, 'efficient': 1, 'administration': 1, 'adequate': 1, 'host': 1, 'resource': 1, 'fine': 1, 'tuning': 1, 'sale': 1, 'effort': 1, 'aid': 1, 'digital': 2, 'trace': 2, 'unique': 1, 'set': 1, 'traceable': 1, 'activitiespage': 1, 'display': 2, 'short': 3, 'description': 3, 'redirect': 2, 'target': 2, 'ingest': 1, 'xml': 5, 'tracing': 1, 'compare': 1, 'security': 2, 'delarosa': 1, 'alexander': 1, 'february': 5, 'monitoring': 1, 'ugly': 1, 'sister': 1, 'pandora': 1, 'fms': 1, 'archive': 2, 'original': 2, 'retrieve': 3, 'register': 1, 'produce': 1, 'stamp': 1, 'documentation': 2, 'behavior': 1, 'condition': 1, 'relevant': 1, 'particular': 1, 'peters': 1, 'thomas': 1, 'library': 2, 'hi': 1, 'tech': 1, 'issn': 3, 'rice': 1, 'ronald': 1, 'borgman': 1, 'christine': 1, 'science': 3, 'research': 2, 'journal': 1, 'american': 1, 'society': 1, 'gerhards': 1, 'march': 3, 'protocol': 1, 'working': 1, 'group': 1, 'propose': 1, 'obsoletes': 1, 'winscp': 1, 'june': 2, 'files': 2, 'codeproject': 1, 'august': 1, 'turn': 1, 'searchable': 1, 'regex': 1, 'classes': 1, 'viewer': 1, 'sql': 5, 'b': 1, 'extended': 1, 'stankovic': 1, 'ivan': 1, 'beginner': 1, 'guide': 1, 'importance': 1, 'techrepublic': 1, 'november': 1, 'logfiles': 1, 'jansen': 1, 'bernard': 1, 'elsevi': 1, 'bv': 1, 'logfile': 1, 'class': 1, 'microsoft': 1, 'sqlserver': 1, 'smo': 1, 'brandom': 1, 'russell': 1, 'january': 1, 'iran': 1, 'block': 1, 'app': 1, 'amid': 1, 'nationwide': 1, 'protest': 1, 'verge': 1, 'vox': 1, 'media': 1, 'caddy': 2, 'works': 1, 'authority': 1, 'control': 1, 'national': 1, 'germany': 1, 'category': 2, 'errorscomputer': 1, 'logginghidden': 1, 'descriptionshort': 1, 'wikidataarticles': 1, 'referencespage': 1, 'module': 1, 'annotated': 1, 'july': 1, 'utc': 1, 'available': 1, 'creative': 1, 'commons': 1, 'attribution': 1, 'sharealike': 1, 'license': 1, 'term': 2, 'apply': 1, 'agree': 1, 'policy': 2, 'registered': 1, 'trademark': 1, 'foundation': 1, 'non': 1, 'profit': 1, 'organization': 1, 'disclaimer': 1, 'contact': 1, 'conduct': 1, 'statistics': 1, 'cookie': 1, 'statement': 1, 'mobile': 1} [('log', 67), ('server', 19), ('event', 17), ('message', 16), ('system', 16)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'logging': 8,\n",
       "  'computing': 5,\n",
       "  'wikipedia': 5,\n",
       "  'jump': 1,\n",
       "  'content': 6,\n",
       "  'main': 4,\n",
       "  'menu': 2,\n",
       "  'sidebar': 4,\n",
       "  'hide': 4,\n",
       "  'navigation': 1,\n",
       "  'pagecontentscurrent': 1,\n",
       "  'eventsrandom': 1,\n",
       "  'articleabout': 1,\n",
       "  'wikipediacontact': 1,\n",
       "  'contribute': 1,\n",
       "  'helplearn': 1,\n",
       "  'editcommunity': 1,\n",
       "  'portalrecent': 1,\n",
       "  'changesupload': 2,\n",
       "  'filespecial': 1,\n",
       "  'page': 4,\n",
       "  'search': 11,\n",
       "  'appearance': 2,\n",
       "  'donate': 2,\n",
       "  'create': 3,\n",
       "  'account': 2,\n",
       "  'log': 67,\n",
       "  'personal': 1,\n",
       "  'tool': 1,\n",
       "  'pages': 1,\n",
       "  'editor': 1,\n",
       "  'learn': 2,\n",
       "  'contributionstalk': 1,\n",
       "  'types': 2,\n",
       "  'toggle': 3,\n",
       "  'subsection': 1,\n",
       "  'event': 17,\n",
       "  'transaction': 13,\n",
       "  'message': 16,\n",
       "  'server': 19,\n",
       "  'reference': 2,\n",
       "  'table': 2,\n",
       "  'language': 2,\n",
       "  'catalàčeštinadeutschespañolفارسیfrançais한국어հայերենbahasa': 1,\n",
       "  'indonesiaitalianoעבריתқазақшаmagyarnederlands日本語norsk': 1,\n",
       "  'bokmålpolskiportuguêsрусскийsimple': 1,\n",
       "  'englishslovenčinasuomisvenskatürkçeукраїнська粵語中文': 1,\n",
       "  'edit': 2,\n",
       "  'link': 3,\n",
       "  'articletalk': 1,\n",
       "  'english': 1,\n",
       "  'readeditview': 2,\n",
       "  'history': 4,\n",
       "  'tools': 2,\n",
       "  'actions': 1,\n",
       "  'general': 3,\n",
       "  'hererelate': 1,\n",
       "  'filepermanent': 1,\n",
       "  'linkpage': 1,\n",
       "  'informationcite': 1,\n",
       "  'pageget': 1,\n",
       "  'shorten': 1,\n",
       "  'urldownload': 1,\n",
       "  'qr': 1,\n",
       "  'code': 4,\n",
       "  'print': 1,\n",
       "  'export': 1,\n",
       "  'download': 1,\n",
       "  'pdfprintable': 1,\n",
       "  'version': 1,\n",
       "  'project': 1,\n",
       "  'wikimedia': 2,\n",
       "  'commonswikidata': 1,\n",
       "  'item': 1,\n",
       "  'free': 1,\n",
       "  'encyclopedia': 1,\n",
       "  'chronological': 1,\n",
       "  'record': 8,\n",
       "  'computer': 6,\n",
       "  'datum': 6,\n",
       "  'processing': 1,\n",
       "  'operation': 4,\n",
       "  'article': 5,\n",
       "  'need': 3,\n",
       "  'additional': 4,\n",
       "  'citation': 2,\n",
       "  'verification': 1,\n",
       "  'help': 1,\n",
       "  'improve': 1,\n",
       "  'add': 3,\n",
       "  'reliable': 1,\n",
       "  'source': 3,\n",
       "  'unsourced': 1,\n",
       "  'material': 1,\n",
       "  'challenge': 1,\n",
       "  'remove': 2,\n",
       "  'find': 1,\n",
       "  'news': 1,\n",
       "  'newspaper': 1,\n",
       "  'book': 1,\n",
       "  'scholar': 1,\n",
       "  'jstor': 1,\n",
       "  'september': 3,\n",
       "  'confuse': 1,\n",
       "  'data': 4,\n",
       "  'logger': 1,\n",
       "  'act': 1,\n",
       "  'keep': 1,\n",
       "  'occur': 3,\n",
       "  'system': 16,\n",
       "  'problem': 3,\n",
       "  'error': 3,\n",
       "  'broad': 1,\n",
       "  'information': 12,\n",
       "  'current': 1,\n",
       "  'operating': 2,\n",
       "  'software': 9,\n",
       "  'entry': 4,\n",
       "  'monitor': 2,\n",
       "  'understand': 5,\n",
       "  'debug': 1,\n",
       "  'audit': 2,\n",
       "  'particularly': 2,\n",
       "  'important': 1,\n",
       "  'multi': 1,\n",
       "  'user': 16,\n",
       "  'central': 1,\n",
       "  'overview': 1,\n",
       "  'simple': 1,\n",
       "  'case': 3,\n",
       "  'write': 2,\n",
       "  'file': 16,\n",
       "  'call': 1,\n",
       "  'alternatively': 1,\n",
       "  'dedicated': 1,\n",
       "  'management': 5,\n",
       "  'store': 5,\n",
       "  'database': 5,\n",
       "  'different': 4,\n",
       "  'specifically': 1,\n",
       "  'communication': 3,\n",
       "  'collection': 2,\n",
       "  'method': 1,\n",
       "  'automatically': 4,\n",
       "  'capture': 1,\n",
       "  'type': 1,\n",
       "  'time': 5,\n",
       "  'person': 2,\n",
       "  'terminal': 1,\n",
       "  'web': 10,\n",
       "  'searching': 1,\n",
       "  'electronic': 1,\n",
       "  'interaction': 2,\n",
       "  'episode': 1,\n",
       "  'engine': 3,\n",
       "  'framework': 1,\n",
       "  'program': 2,\n",
       "  'include': 3,\n",
       "  'widely': 1,\n",
       "  'standard': 4,\n",
       "  'syslog': 3,\n",
       "  'define': 1,\n",
       "  'ietf': 1,\n",
       "  'rfc': 3,\n",
       "  'enable': 2,\n",
       "  'dedicate': 1,\n",
       "  'standardized': 1,\n",
       "  'subsystem': 1,\n",
       "  'generate': 1,\n",
       "  'filter': 1,\n",
       "  'analyze': 1,\n",
       "  'relieve': 1,\n",
       "  'developer': 2,\n",
       "  'have': 1,\n",
       "  'design': 2,\n",
       "  'ad': 1,\n",
       "  'hoc': 1,\n",
       "  'take': 1,\n",
       "  'place': 1,\n",
       "  'execution': 1,\n",
       "  'activity': 2,\n",
       "  'diagnose': 1,\n",
       "  'essential': 1,\n",
       "  'application': 2,\n",
       "  'little': 1,\n",
       "  'useful': 1,\n",
       "  'combine': 2,\n",
       "  'multiple': 1,\n",
       "  'combination': 1,\n",
       "  'yield': 1,\n",
       "  'relate': 2,\n",
       "  'solution': 1,\n",
       "  'employ': 1,\n",
       "  'network': 2,\n",
       "  'wide': 1,\n",
       "  'querying': 1,\n",
       "  'maintain': 5,\n",
       "  'kind': 1,\n",
       "  'mainly': 1,\n",
       "  'intend': 2,\n",
       "  'trail': 1,\n",
       "  'later': 1,\n",
       "  'analysis': 5,\n",
       "  'human': 2,\n",
       "  'readable': 3,\n",
       "  'change': 5,\n",
       "  'allow': 1,\n",
       "  'recover': 1,\n",
       "  'crash': 1,\n",
       "  'consistent': 1,\n",
       "  'state': 1,\n",
       "  'usually': 2,\n",
       "  'use': 4,\n",
       "  'intranet': 1,\n",
       "  'site': 3,\n",
       "  'provide': 1,\n",
       "  'valuable': 1,\n",
       "  'insight': 1,\n",
       "  'process': 1,\n",
       "  'online': 1,\n",
       "  'understanding': 1,\n",
       "  'enlighten': 1,\n",
       "  'interface': 1,\n",
       "  'development': 2,\n",
       "  'devise': 1,\n",
       "  'architecture': 2,\n",
       "  'internet': 3,\n",
       "  'relay': 1,\n",
       "  'chat': 5,\n",
       "  'irc': 4,\n",
       "  'instant': 2,\n",
       "  'messaging': 2,\n",
       "  'im': 3,\n",
       "  'peer': 2,\n",
       "  'share': 1,\n",
       "  'client': 4,\n",
       "  'function': 1,\n",
       "  'multiplayer': 1,\n",
       "  'game': 1,\n",
       "  'especially': 1,\n",
       "  'mmorpg': 1,\n",
       "  'commonly': 1,\n",
       "  'ability': 1,\n",
       "  'save': 2,\n",
       "  'textual': 2,\n",
       "  'public': 2,\n",
       "  'channel': 4,\n",
       "  'conference': 1,\n",
       "  'mmo': 1,\n",
       "  'party': 1,\n",
       "  'private': 1,\n",
       "  'universally': 1,\n",
       "  'plain': 1,\n",
       "  'text': 3,\n",
       "  'voip': 2,\n",
       "  'support': 1,\n",
       "  'skype': 1,\n",
       "  'html': 1,\n",
       "  'custom': 1,\n",
       "  'format': 6,\n",
       "  'ease': 1,\n",
       "  'read': 1,\n",
       "  'encryption': 1,\n",
       "  'topic': 2,\n",
       "  'join': 1,\n",
       "  'exit': 1,\n",
       "  'kick': 1,\n",
       "  'ban': 1,\n",
       "  'nickname': 1,\n",
       "  'status': 1,\n",
       "  'make': 1,\n",
       "  'like': 1,\n",
       "  'combined': 1,\n",
       "  'question': 1,\n",
       "  'comparable': 1,\n",
       "  'true': 1,\n",
       "  'visible': 1,\n",
       "  'frame': 1,\n",
       "  'spend': 1,\n",
       "  'connect': 1,\n",
       "  'certain': 1,\n",
       "  'offer': 1,\n",
       "  'chance': 1,\n",
       "  'encrypt': 2,\n",
       "  'enhance': 1,\n",
       "  'privacy': 4,\n",
       "  'require': 1,\n",
       "  'password': 1,\n",
       "  'decrypt': 1,\n",
       "  'view': 2,\n",
       "  'handle': 1,\n",
       "  'respective': 1,\n",
       "  'writing': 1,\n",
       "  'focus': 2,\n",
       "  'service': 3,\n",
       "  'signal': 1,\n",
       "  'minimal': 1,\n",
       "  'limit': 1,\n",
       "  'connection': 1,\n",
       "  'apache': 1,\n",
       "  'access': 2,\n",
       "  'show': 1,\n",
       "  'wordpress': 1,\n",
       "  'vulnerability': 1,\n",
       "  'bot': 1,\n",
       "  'consist': 1,\n",
       "  'list': 1,\n",
       "  'perform': 1,\n",
       "  'typical': 1,\n",
       "  'example': 1,\n",
       "  'request': 4,\n",
       "  'common': 1,\n",
       "  'proprietary': 1,\n",
       "  'json': 1,\n",
       "  'versus': 1,\n",
       "  'recent': 1,\n",
       "  'typically': 3,\n",
       "  'append': 1,\n",
       "  'end': 1,\n",
       "  'ip': 1,\n",
       "  'address': 1,\n",
       "  'date': 1,\n",
       "  'http': 1,\n",
       "  'bytes': 1,\n",
       "  'serve': 1,\n",
       "  'agent': 2,\n",
       "  'referrer': 3,\n",
       "  'single': 1,\n",
       "  'separate': 1,\n",
       "  'distinct': 1,\n",
       "  'collect': 1,\n",
       "  'specific': 1,\n",
       "  'accessible': 1,\n",
       "  'webmaster': 1,\n",
       "  'administrative': 1,\n",
       "  'statistical': 1,\n",
       "  'examine': 1,\n",
       "  'traffic': 1,\n",
       "  'pattern': 1,\n",
       "  'day': 2,\n",
       "  'week': 1,\n",
       "  'efficient': 1,\n",
       "  'administration': 1,\n",
       "  'adequate': 1,\n",
       "  'host': 1,\n",
       "  'resource': 1,\n",
       "  'fine': 1,\n",
       "  'tuning': 1,\n",
       "  'sale': 1,\n",
       "  'effort': 1,\n",
       "  'aid': 1,\n",
       "  'digital': 2,\n",
       "  'trace': 2,\n",
       "  'unique': 1,\n",
       "  'set': 1,\n",
       "  'traceable': 1,\n",
       "  'activitiespage': 1,\n",
       "  'display': 2,\n",
       "  'short': 3,\n",
       "  'description': 3,\n",
       "  'redirect': 2,\n",
       "  'target': 2,\n",
       "  'ingest': 1,\n",
       "  'xml': 5,\n",
       "  'tracing': 1,\n",
       "  'compare': 1,\n",
       "  'security': 2,\n",
       "  'delarosa': 1,\n",
       "  'alexander': 1,\n",
       "  'february': 5,\n",
       "  'monitoring': 1,\n",
       "  'ugly': 1,\n",
       "  'sister': 1,\n",
       "  'pandora': 1,\n",
       "  'fms': 1,\n",
       "  'archive': 2,\n",
       "  'original': 2,\n",
       "  'retrieve': 3,\n",
       "  'register': 1,\n",
       "  'produce': 1,\n",
       "  'stamp': 1,\n",
       "  'documentation': 2,\n",
       "  'behavior': 1,\n",
       "  'condition': 1,\n",
       "  'relevant': 1,\n",
       "  'particular': 1,\n",
       "  'peters': 1,\n",
       "  'thomas': 1,\n",
       "  'library': 2,\n",
       "  'hi': 1,\n",
       "  'tech': 1,\n",
       "  'issn': 3,\n",
       "  'rice': 1,\n",
       "  'ronald': 1,\n",
       "  'borgman': 1,\n",
       "  'christine': 1,\n",
       "  'science': 3,\n",
       "  'research': 2,\n",
       "  'journal': 1,\n",
       "  'american': 1,\n",
       "  'society': 1,\n",
       "  'gerhards': 1,\n",
       "  'march': 3,\n",
       "  'protocol': 1,\n",
       "  'working': 1,\n",
       "  'group': 1,\n",
       "  'propose': 1,\n",
       "  'obsoletes': 1,\n",
       "  'winscp': 1,\n",
       "  'june': 2,\n",
       "  'files': 2,\n",
       "  'codeproject': 1,\n",
       "  'august': 1,\n",
       "  'turn': 1,\n",
       "  'searchable': 1,\n",
       "  'regex': 1,\n",
       "  'classes': 1,\n",
       "  'viewer': 1,\n",
       "  'sql': 5,\n",
       "  'b': 1,\n",
       "  'extended': 1,\n",
       "  'stankovic': 1,\n",
       "  'ivan': 1,\n",
       "  'beginner': 1,\n",
       "  'guide': 1,\n",
       "  'importance': 1,\n",
       "  'techrepublic': 1,\n",
       "  'november': 1,\n",
       "  'logfiles': 1,\n",
       "  'jansen': 1,\n",
       "  'bernard': 1,\n",
       "  'elsevi': 1,\n",
       "  'bv': 1,\n",
       "  'logfile': 1,\n",
       "  'class': 1,\n",
       "  'microsoft': 1,\n",
       "  'sqlserver': 1,\n",
       "  'smo': 1,\n",
       "  'brandom': 1,\n",
       "  'russell': 1,\n",
       "  'january': 1,\n",
       "  'iran': 1,\n",
       "  'block': 1,\n",
       "  'app': 1,\n",
       "  'amid': 1,\n",
       "  'nationwide': 1,\n",
       "  'protest': 1,\n",
       "  'verge': 1,\n",
       "  'vox': 1,\n",
       "  'media': 1,\n",
       "  'caddy': 2,\n",
       "  'works': 1,\n",
       "  'authority': 1,\n",
       "  'control': 1,\n",
       "  'national': 1,\n",
       "  'germany': 1,\n",
       "  'category': 2,\n",
       "  'errorscomputer': 1,\n",
       "  'logginghidden': 1,\n",
       "  'descriptionshort': 1,\n",
       "  'wikidataarticles': 1,\n",
       "  'referencespage': 1,\n",
       "  'module': 1,\n",
       "  'annotated': 1,\n",
       "  'july': 1,\n",
       "  'utc': 1,\n",
       "  'available': 1,\n",
       "  'creative': 1,\n",
       "  'commons': 1,\n",
       "  'attribution': 1,\n",
       "  'sharealike': 1,\n",
       "  'license': 1,\n",
       "  'term': 2,\n",
       "  'apply': 1,\n",
       "  'agree': 1,\n",
       "  'policy': 2,\n",
       "  'registered': 1,\n",
       "  'trademark': 1,\n",
       "  'foundation': 1,\n",
       "  'non': 1,\n",
       "  'profit': 1,\n",
       "  'organization': 1,\n",
       "  'disclaimer': 1,\n",
       "  'contact': 1,\n",
       "  'conduct': 1,\n",
       "  'statistics': 1,\n",
       "  'cookie': 1,\n",
       "  'statement': 1,\n",
       "  'mobile': 1},\n",
       " [('log', 67), ('server', 19), ('event', 17), ('message', 16), ('system', 16)])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_count(example_processed_data[3], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Define the following methods:\n",
    "    * `score_sentence_by_token(sentence, interesting_token)` that takes a sentence and a list of interesting token and returns the number of times that any of the interesting words appear in the sentence divided by the number of words in the sentence\n",
    "    * `score_sentence_by_lemma(sentence, interesting_lemmas)` that takes a sentence and a list of interesting lemmas and returns the number of times that any of the interesting lemmas appear in the sentence divided by the number of words in the sentence\n",
    "    \n",
    "You may find some of the code from the in class notes useful; feel free to use methods (rewrite them in this cell as well).  Test them by showing the score of the first sentence in your article using the frequent tokens and frequent lemmas identified in question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence_by_token(sentence, interesting_token, nlp_model = \"en_core_web_sm\"):\n",
    "    #Initialize language and doc\n",
    "    nlp = spacy.load(nlp_model)\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    all_tokens = [token.text.lower() for token in doc if token.is_alpha]\n",
    "    hits = sum(1 for token in all_tokens if token in interesting_token)\n",
    "\n",
    "    total_tokens = len(all_tokens)\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    score = hits/total_tokens\n",
    "    return score\n",
    "\n",
    "def score_sentence_by_lemma(sentence, interesting_lemma, nlp_model = \"en_core_web_sm\"):\n",
    "    #Initialize language and doc\n",
    "    nlp = spacy.load(nlp_model)\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    all_lemmas = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "    hits = sum(1 for token in all_lemmas if token in interesting_lemma)\n",
    "\n",
    "    total_lemmas = len(all_lemmas)\n",
    "\n",
    "    if total_lemmas == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    score = hits/total_lemmas\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Make a list containing the scores (using tokens) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores. From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Make a list containing the scores (using lemmas) of every sentence in the article, and plot a histogram with appropriate titles and axis labels of the scores.  From your histogram, what seems to be the most common range of scores (put the answer in a comment after your code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Which tokens and lexems would be ommitted from the lists generated in questions 3 and 4 if we only wanted to consider nouns as interesting words?  How might we change the code to only consider nouns? Put your answer in this Markdown cell (you can edit it by double clicking it)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
